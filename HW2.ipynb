{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Part_of_speech</th>\n",
       "      <th>Chunking</th>\n",
       "      <th>NER_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>further</td>\n",
       "      <td>JJ</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scientific</td>\n",
       "      <td>JJ</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>study</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word Part_of_speech Chunking NER_tag\n",
       "0          He            PRP     I-NP       O\n",
       "1        said            VBD     I-VP       O\n",
       "2     further             JJ     I-NP       O\n",
       "3  scientific             JJ     I-NP       O\n",
       "4       study             NN     I-NP       O"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I noticed that there is a certain string that is pervasive throughout the dataset\n",
    "#  (\"-DOCSTART- -X- O O\") indicating document divisions, so I went through and \n",
    "#  removed all instances of it in the txt file so that they would not show up in \n",
    "#  the data frame rows when I read it in using read_csv.\n",
    "with open(\"DataNer.txt\", \"r+\") as f:\n",
    "    d = f.readlines()\n",
    "    f.seek(0)\n",
    "    for i in d:\n",
    "        if i != \"-DOCSTART- -X- O O\":\n",
    "            f.write(i)\n",
    "    f.truncate()\n",
    "\n",
    "# Read in the dataset with correct column names\n",
    "import pandas as pd\n",
    "colnames = ['Word', 'Part_of_speech', 'Chunking', 'NER_tag']\n",
    "data2 = pd.read_csv(\"DataNer.txt\", names = colnames, delimiter = \" \")\n",
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Parse through txt file to create a list of sentence numbers\n",
    "import itertools\n",
    "sentence_num_list = []\n",
    "curr_sentence_num = 1\n",
    "with open('DataNer.txt') as f:\n",
    "    for line in f:\n",
    "        #print(type(line))\n",
    "        #print(line)\n",
    "        if len(line.strip()) == 0:\n",
    "            curr_sentence_num = curr_sentence_num + 1\n",
    "        else:\n",
    "            sentence_num_list.append(curr_sentence_num)\n",
    "    print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Part_of_speech</th>\n",
       "      <th>Chunking</th>\n",
       "      <th>NER_tag</th>\n",
       "      <th>Sentence_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He</td>\n",
       "      <td>PRP</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>said</td>\n",
       "      <td>VBD</td>\n",
       "      <td>I-VP</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>further</td>\n",
       "      <td>JJ</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scientific</td>\n",
       "      <td>JJ</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>study</td>\n",
       "      <td>NN</td>\n",
       "      <td>I-NP</td>\n",
       "      <td>O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Word Part_of_speech Chunking NER_tag  Sentence_num\n",
       "0          He            PRP     I-NP       O             1\n",
       "1        said            VBD     I-VP       O             1\n",
       "2     further             JJ     I-NP       O             1\n",
       "3  scientific             JJ     I-NP       O             1\n",
       "4       study             NN     I-NP       O             1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the list created above as a column of sentence number in our dataframe\n",
    "data2.insert(4, \"Sentence_num\", sentence_num_list)\n",
    "data2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any NA entries and vectorize predictor variables in the dataset\n",
    "# Here I separated the predictors from the y-values and created two different matrices. For\n",
    "#  the predictors' matrix (X), I used the to_dict function to create a numerical matrix with\n",
    "#  the columns containing the counts of each word and the rows referring to each of the \n",
    "#  individual words in our dataset. This was done because the first class of models that\n",
    "#  I built below (SGD, Multinomial Naive Bayes, and Perceptron classifier) take in numerical\n",
    "#  matrices as opposed to characters/strings that we have in our dataframe above.\n",
    "\n",
    "data2 = data2.fillna(method='ffill')\n",
    "data2.head()\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "X1 = data2.drop('NER_tag', axis=1)\n",
    "DictVector = DictVectorizer(sparse=True)\n",
    "X = DictVector.fit_transform(X1.to_dict('records'))\n",
    "y = data2.NER_tag.values   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y above into training and testing set:\n",
    "# As specified in the assignment, I used 70% of the dataset as the training set and \n",
    "# 30% for the testing set.  These sets were used directly for the first three models below.\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description of the model building/training/testing/evaluation process for the 1st class \n",
    "# of models (SGD, MNB, and Perceptron):\n",
    "#  Since using cross validation and grid search to find the best values for \n",
    "#  parameters/hyperparameters for these models would take too long, I used the default\n",
    "#  values.  I fit each of the models on the training set I created above, and then predicted\n",
    "#  the target values for the testing set (X_test).  I compared the predicted values \n",
    "#  to the test set's actual values to compute the weighted f1 score. I evaluated the models\n",
    "#  based on this score and also viewed a classification report for each of the models \n",
    "#  so that I could see the precision and recall for each of the 8 target value groups.\n",
    "#  Note that when calculating the f1 scores, I did not remove the 'O' tags, \n",
    "#  as they are abundant in real life (and in datasets apart from the testing one \n",
    "#  used here), and I wanted to evaluate the model's ability to predict \n",
    "#  all types tags (even among noise that that 'O' tags bring).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorsamassihpour/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.7558173297713038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorsamassihpour/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         3\n",
      "      B-MISC       0.00      0.00      0.00        11\n",
      "       B-ORG       0.00      0.00      0.00         5\n",
      "       I-LOC       0.00      0.00      0.00      2574\n",
      "      I-MISC       0.00      0.00      0.00      1391\n",
      "       I-ORG       0.00      0.00      0.00      3057\n",
      "       I-PER       1.00      0.00      0.00      3399\n",
      "           O       0.83      1.00      0.91     51741\n",
      "\n",
      "    accuracy                           0.83     62181\n",
      "   macro avg       0.23      0.12      0.11     62181\n",
      "weighted avg       0.75      0.83      0.76     62181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SGD (Stochastic Gradient Descent) linear classifier:\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "sgd = linear_model.SGDClassifier()\n",
    "sgd.partial_fit(X_train, y_train, classes=np.unique(y))\n",
    "result_y = sgd.predict(X_test)\n",
    "# Can remove all 'O' entries before calculating the f-score (Did not do this as explained below)\n",
    "#labels = list(result_y)\n",
    "#labels.remove('O')\n",
    "print(\"f1 score:\", f1_score(y_test, result_y, average='weighted'))\n",
    "\n",
    "#classes = [i for i in list(data2.NER_tag.unique()) if i != 'O']   \n",
    "print(metrics.classification_report(y_test, result_y))\n",
    "\n",
    "# Note:\n",
    "#  For the first three models, I was getting a dead kernel so I took the posted advice and \n",
    "#  used partial_fit instead of fit.  I then got an error and found that I needed to \n",
    "#  specify the \"classes\" parameter in partial_fit, which I set to classes=np.unique(y) ,\n",
    "#  which specified the number of unique classes that we have.  \n",
    "#  Then I predicted on the test set and printed the f1 score and got a warning \n",
    "#  : \"UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no \n",
    "#  predicted samples.  'precision', 'predicted', average, warn_for).\"\"\n",
    "#  I searched on google and found that this is because some of the results in the test \n",
    "#  set are not in the predicted set, and thus the f score for these cases is 0, and they \n",
    "#  are included in the calculation of the average f1 score.\n",
    "#  Thus, in the f1_score() function, I could have\n",
    "#  the parameter 'labels=np.unique(result_y)' to get rid of the labels that were not \n",
    "#  predicted set, so that these zero values are not included in the average calculation.  \n",
    "#  But in lecture it was said that we should not do this, so I did not include this.\n",
    "\n",
    "\n",
    "# A previous output gave a difference precision for the I-MISC tag.\n",
    "#f1 score: 0.7534990387203918\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#        B-LOC       0.00      0.00      0.00         5\n",
    "#       B-MISC       0.00      0.00      0.00        10\n",
    "#        B-ORG       0.00      0.00      0.00        10\n",
    "#        I-LOC       0.00      0.00      0.00      2565\n",
    "#       I-MISC       0.50      0.00      0.00      1403\n",
    "#        I-ORG       0.00      0.00      0.00      3119\n",
    "#        I-PER       0.00      0.00      0.00      3434\n",
    "#            O       0.83      1.00      0.91     51635\n",
    "\n",
    "#    micro avg       0.83      0.83      0.83     62181\n",
    "#    macro avg       0.17      0.13      0.11     62181\n",
    "# weighted avg       0.70      0.83      0.75     62181\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.9213951759106611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorsamassihpour/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.00      0.00      0.00         5\n",
      "      B-MISC       0.00      0.00      0.00        10\n",
      "       B-ORG       0.00      0.00      0.00         4\n",
      "       I-LOC       0.89      0.62      0.73      2456\n",
      "      I-MISC       0.65      0.19      0.30      1490\n",
      "       I-ORG       0.77      0.63      0.70      3035\n",
      "       I-PER       0.73      0.82      0.77      3406\n",
      "           O       0.95      0.99      0.97     51775\n",
      "\n",
      "    accuracy                           0.93     62181\n",
      "   macro avg       0.50      0.41      0.43     62181\n",
      "weighted avg       0.92      0.93      0.92     62181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multinomial Naive Bayes Model Classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.partial_fit(X_train, y_train, classes=np.unique(y))\n",
    "result_y = mnb.predict(X_test)\n",
    "print(\"f1 score:\", f1_score(y_test, result_y, average='weighted', labels=np.unique(result_y)))\n",
    "print(metrics.classification_report(y_test, result_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.8583100132983884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-ORG       0.05      0.00      0.00      3035\n",
      "           O       0.83      1.00      0.91     51775\n",
      "\n",
      "   micro avg       0.83      0.94      0.88     54810\n",
      "   macro avg       0.44      0.50      0.46     54810\n",
      "weighted avg       0.79      0.94      0.86     54810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perceptron Classifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "perc = Perceptron()\n",
    "perc.partial_fit(X_train, y_train, classes=np.unique(y))\n",
    "result_y = perc.predict(X_test)\n",
    "print(\"f1 score:\", f1_score(y_test, result_y, average='weighted', labels=np.unique(result_y)))\n",
    "print(metrics.classification_report(y_test, result_y, labels=np.unique(result_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing the weighted f1 scores for the first three models, we can see that the \n",
    "#  Multinomial Naive Bayes performs the best as it has the highest score at about .919. \n",
    "#  (We are looking for the highest f1 score:\n",
    "#  The higher the f-score, the better the precision and recall of the model, or the \n",
    "#  higher the proportion of true positives over the combination of true and false positives, \n",
    "#  and the proportion of true positives over the combination of true positives and false \n",
    "#  negatives, respectively.)\n",
    "#  The next best performing is the perceptron with an f1 score of .857, and the SGD has an\n",
    "#  f score of about .758, which is still relatively somewhat strong.\n",
    "#  In the classification reports, we can see the individual precision and recall for each\n",
    "#  of the different classes of NER tags. Aside from the 'O' tag which are prevalent and \n",
    "#  can cause noise in the dataset, the Multinomial Naive Bayes has the highest precision for\n",
    "#  'I-LOC' group (.9) and the highest recall for the 'I-PER' group (.79), and so it can \n",
    "#  predict these classes of NER tags well.  Though not important, we can see that all three \n",
    "#  the models above do well in predicting 'O' tags, mainly because they are so prevalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Random Fields (CRF)\n",
    "\n",
    "# Find features to include in the CRF model:\n",
    "#  CRF models use specific features to predict the probability of the target label \n",
    "#  sequence (NER tag).\n",
    "#  Here I specifically looked at whether a word was lowercase, uppercase, a digit, or a \n",
    "#  title, and nearby words. The functions below return a dictionary for each sentence, \n",
    "#  where each feature is a dictionary value.\n",
    "\n",
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'postag': postag,\n",
    "        'postag[:2]': postag[:2],\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[:2]': postag1[:2],\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Word Part_of_speech Chunking NER_tag  Sentence_num\n",
      "145089      the             DT     I-NP       O         11192\n",
      "145090     Cubs            NNP     I-NP   I-ORG         11192\n",
      "145091        '            POS     B-NP       O         11192\n",
      "145092  playoff             NN     I-NP       O         11192\n",
      "145093    hopes            NNS     I-NP       O         11192\n",
      "...         ...            ...      ...     ...           ...\n",
      "145284      1-0             JJ     I-NP       O         11201\n",
      "145285     down             JJ     I-NP       O         11201\n",
      "145286    after             IN     I-PP       O         11201\n",
      "145287     four             CD     I-NP       O         11201\n",
      "145288  minutes            NNS     I-NP       O         11201\n",
      "\n",
      "[200 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a training and test set:\n",
    "# len(data2) is equal to 207270 (number of words) and we want to put 207270*(.7) = 145089 \n",
    "#  or 70% into the training set\n",
    "\n",
    "training_crf = data2.iloc[0:145089,]\n",
    "testing_crf = data2.iloc[145089:,]\n",
    "\n",
    "print(testing_crf.head(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Create the right formats for the training and testing sets so they are compatible to be\n",
    "#  put through the above functions to create features:\n",
    "#  I am creating a list of list of tuples for the training set and \n",
    "#  testing set.  Each tuple contains a row of the original\n",
    "#  dataframe, and the tuple is an element of a list containing tuples (words) that are all  \n",
    "#  in the same sentence.  This list is then part of the outer list of all sentences in the\n",
    "#  original data (split between training and testing data).\n",
    "\n",
    "sentence_num_counter = 1\n",
    "outer_list_training = []\n",
    "curr_same_sentence_list = []\n",
    "for index, row in training_crf.iterrows():\n",
    "    curr_sentence_num = row['Sentence_num']\n",
    "    if (index > 0):\n",
    "        if (len(curr_list) != 0):\n",
    "            curr_same_sentence_list.append(curr_list)\n",
    "    if (curr_sentence_num == sentence_num_counter):\n",
    "        curr_list = tuple([row['Word'], row['Part_of_speech'], row['NER_tag']])\n",
    "        curr_same_sentence_list.append(curr_list)\n",
    "        curr_list = []\n",
    "    else:\n",
    "        outer_list_training.append(curr_same_sentence_list)\n",
    "        sentence_num_counter = sentence_num_counter + 1\n",
    "        curr_list = tuple([row['Word'], row['Part_of_speech'], row['NER_tag']])\n",
    "        curr_same_sentence_list=[]\n",
    "#print(outer_list_training[0:3])\n",
    "#print(\"\\n\")\n",
    "\n",
    "sentence_num_counter = 11192\n",
    "outer_list_testing = []\n",
    "curr_same_sentence_list = []\n",
    "for index, row in testing_crf.iterrows():\n",
    "    curr_sentence_num = row['Sentence_num']\n",
    "    if (index > 0):\n",
    "        if (len(curr_list) != 0):\n",
    "            curr_same_sentence_list.append(curr_list)\n",
    "    if (curr_sentence_num == sentence_num_counter):\n",
    "        curr_list = tuple([row['Word'], row['Part_of_speech'], row['NER_tag']])\n",
    "        curr_same_sentence_list.append(curr_list)\n",
    "        curr_list = []\n",
    "    else:\n",
    "        outer_list_testing.append(curr_same_sentence_list)\n",
    "        curr_list = tuple([row['Word'], row['Part_of_speech'], row['NER_tag']])\n",
    "        curr_same_sentence_list=[]\n",
    "\n",
    "print(\"done\")\n",
    "#print(outer_list_testing[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Conver our training and testing set into a list of dictionaries using\n",
    "#  the functions above where each dictionary\n",
    "#  corresponds to a sentence and the dictionary keys correspond to a feature. \n",
    "\n",
    "# Please note that in the above step, I ended up not including the chunking or sentence \n",
    "#  number columns as the functions below would not finish when I included them.  Thus,\n",
    "#  I decided to keep the three columns I thought were most important, mainly the word, \n",
    "#  the part of speech, and the NER tag.\n",
    "\n",
    "X_train = [sent2features(s) for s in outer_list_training]\n",
    "y_train = [sent2labels(s) for s in outer_list_training]\n",
    "\n",
    "X_test = [sent2features(s) for s in outer_list_testing]\n",
    "y_test = [sent2labels(s) for s in outer_list_testing]\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Train the CRF model on our training set (X_train):\n",
    "#  Similar to the process I used when building evaluating the first three models, I\n",
    "#  fit the CRF model on the training set created above, and then predicted the target\n",
    "#  values for the testing set.  I compared these predicted values to the test set's \n",
    "#  actual values in order to calculate the f1 score, with which I used to evaluate the \n",
    "#  CRF's performance among those of the other models. I again used default parameters\n",
    "#  for the model, which I explicitly wrote out below. To be consistent with the other\n",
    "#  models above, I did not remove the 'O' labels, for the same reason explained \n",
    "#  previously.  \n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import sklearn\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.9355090635291945\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the CRF model:\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "print(\"f1 score:\", (metrics.flat_f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dorsamassihpour/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/dorsamassihpour/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC      0.000     0.000     0.000         1\n",
      "       I-LOC      0.884     0.830     0.856      2444\n",
      "      B-MISC      0.000     0.000     0.000         8\n",
      "      I-MISC      0.894     0.735     0.806      1429\n",
      "       B-ORG      0.000     0.000     0.000         0\n",
      "       I-ORG      0.802     0.764     0.783      2809\n",
      "       I-PER      0.842     0.908     0.874      2743\n",
      "\n",
      "   micro avg      0.847     0.818     0.832      9434\n",
      "   macro avg      0.489     0.462     0.474      9434\n",
      "weighted avg      0.848     0.818     0.831      9434\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the NER tags and look at the flat classification report\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))\n",
    "\n",
    "# By looking at the f1 score, we can see that this model performs well,\n",
    "#  with a score of .936.  It outperforms the Multinomial Naive Bayes model, the\n",
    "#  Perceptron model, and the CRF model.  Looking at the flat classification report, \n",
    "#  we can see that the precision for predicting I-LOC (.884) and I-MISC (.894) \n",
    "#  are the highest among all NER tags with this CRF model, and the recall is highest \n",
    "#  for the I-PER tag (.908).  Thus, the model is able to make the best predidctions for\n",
    "#  these categories of NER tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Citations for sources used:\n",
    "\n",
    "# “Tutorial¶.” Sklearn, https://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html.\n",
    "\n",
    "# “Depends on the Definition.” Depends on the Definition - It's about Machine Learning, \n",
    "#  Data Science and More, https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/.\n",
    "\n",
    "# Safdari, Nasir. “Named Entity Recognition (NER), Meeting Industry's Requirement by \n",
    "#  Applying State-of-the-Art Deep...” Medium, Towards Data Science, 12 Dec. 2018, \n",
    "#  https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
